## Assumption of Linear Regression
1. Linearity: Relationship between independent variable $(x)$ and dependent variable $(y)$ must be linear. we can check this using the scatter plot. we should be seeing the random cloud of data points. if we will see the U shaped or curved data point then it is voilating the assumption. we can fix this by doing the transformation like log, $x^2, x^3$  
    - we can create the Residual vs Fitted plot in which x-axis will have the predicted values $y^{hat}$ vs residuals which is $y^i-y^{hat}$. if this plot show the random clouds of points then we are able to capture all the underline pattern in the data if not then we were not able to capture the curve of the data.
    - why we plot the $y^{hat}$, because we summarize all the input variables into the single variable, instead of having 20 plots for individual variable we use one variable to check the model wise performance.
2. Homoscedasticity (Constant variance) : The spread of residuals should be constant, it should not increase or decrease as we move from left to right. If does changes then its standard error and p value is not reliable.
    - we can check residuals plotss for funnel or cone shaped plot, if it is present then it is hetroscedasticity.
    - we have statistical tests also like breush-Pagan or Goldfeld-Quandt.
3. No Auto-correlation : The error of the one observation should not be correlated to error of another observation. It is common in time series data  
    - If errors are correlated then model we will double count the information leading to artificial low p values (making variables more significant then they are)  
    - we can check this by durbin-watson test, the values ranges from 0 to 4. if the values is 2 then no autocorrelation, if 0 then positive and negative if values near 4.
4. Normality of Residuals : The residual should flow the normal distribution. this assumption is required for reliable hypothesis Testing (p-values) and confidence interval. The features themselves dont need to be normal only the errors. 
    - we can check using the Q-Q plot, histogram of erros should be in bell shaped, shapiro-wilk test or jarque-bera test.
5. No or little Multicollinearity : The independent variable shoudl not be highly correlated with each other. Otherwise it will make impossible for the model to distinguish the individual effect of each feature. coefficients becomes unstable and swing widly.
    - we can check the correlation matrix and look for values > 0.8
    - we can also use the VIF which is variance inflation factor, if value is > 5 or 10 that means we have problematic multicollinearity.
    - we can remove any highly correlated variables, combine them, or use the regularization technique like Lasso, ridge. 
    