## Regression Metrics
1. MAE : Mean absolute Error, $(\sum|y_i - x_i|)/n$ It is used when we have very dirty data and has many outliers so that we dont want to over influence the model.
2. MSE :$(\sum|y_i - x_i|)^2/n$ when we want to heavenly penalize the large errors. It is the standard loss function for training the models. This metric is very sensitive to outliers, single huge error will explode the MSE value 
3. Root Mean squared Error : $\sqrt{(\sum|y_i - x_i|)^2/n}$, when we want the same penalty but with interpretable unit then we use the RMSE. Still it is sensitive to the outliers.
4. MAPE : Mean absolute percentage error, it is used to tell the manager how percentage we are correct or how off we are from the desired value.  
Ques : Can we use the MSE for logistic regression ?  
Ans : Technically yes but practically no, if we use the mse with sigmoid function then our loss function will become the non convex fucntion due to it will get stuck in the local minima and wont be able to find the best solution   
## Logistic Metrics 
1. For the Logistic regression we use the Log Loss (Binary Cross Entropy) for training. For the classification problem we use the different metrics 
    - Accuracy : overall correctness of the model. it is interpretable score.
    - Precision/Recall : Important when the classes are imbalanced and we our more incline for the certain types of the results.
    - F1- Score : Harmonic mean of the precision and recall. it is also called interpretable metric 
    $F1-score = 2*(precision*recall)/(precision + recall )$  
    - ROC AUC Curve = It helps us understand how well the model distinguish between classes.
## Core Evaluation Metrics
1. Accuracy : $(TP+TN)/ Total$, How often model is predicting correctly weather Postives or Negatives.
2. Precision : $TP/(TP+FP)$, of all predicted positives, how many were actually positives. 
3. Recall : $TP/(TP + FN)$, of all the predicted values how many we actually able to catch correct. 
4. F1-Score : what is the balance between precision and recall, this is for imbalanced dataset.  
It punishes extreme values if precision is 1.0 and Recall is 0.0, the F1-score will be 0. alerting you that model is failing.
## Probability Based Metrics
Logistic regression predict probabilties, we want to check the quality of those probabilities.
### ROC-AUC Plot
Receiver operating characteristic and area under curve. It plots the True Positives rate vs False positives rate. It is scale invariant and thershold invariant. It tells you good your model is at distinguishing the classes regardless the thereshold.  
