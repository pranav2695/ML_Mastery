## Logistic Regression
- Logistic Regression in a way is the extension of linear regression, using the thresholds we can classify into classes which is called decision boundry.
$g(z) = 1/(1 + e^{-z}$)  
$z = f_{w,b}(x)$ 
- Cost function of logistic regression is not same as used for the linear regression because it will be non convex function for logistinc regression due to that it will sucked into one of the local minima.
  
  $L(f_{w,b}(x^i, y^i)) = -y^ilog(f_{w,b}(x^i))-(1-y^i)(log(1-f_{w,b}(x^i)))$
- overfitting/underfitting : when the model is not very good fit of the model then it is called underfitting, they have high bias. when the model fit the data too well which learning the noise also then it is called overfitting, model have high variance.
- Technique to solve the issue of overfitting  
    1. Collect more training data 
    2. select the feature, this is called feature selection
## Regularization
- It is the technique to prevent the overfitting by adding the penalty of complexity, regularization keeps the model simple and improve its ability to generalize to new data.
1. Ridge (L2) adds a penalty equal to the square of the magnitude of the coefficients. it adds $\lambda\sum\theta_j^2$ to the cost function. It will shrink the coefficients toward zero but will never make them exact zero.   
    a. It is used when we suspect there are many features which have very least effect on output. it is particulary effective in multicollinearity. where input features are highly correlated.
2. Lasso (L1) least absolute shrinkage and selection operator add a penalty $\lambda\sum|\theta_j|$ to the cost function. It can shrink the some coefficient to the exact zero.  
    a. it is used when you have high number of features and you suspect only few are important. it acts as a in build feature selection tool, creating a sparse and more interpretable model.
3. Elastic Net is the hybrid approach which will add both L1 and L2 technique together $\lambda_2\sum\theta_j^2$ + $\lambda_1\sum|\theta_j|$  
    a. when you have multiple features that are correlated with each other. In such cases, Lasso might randomly pick one feature from a group of correlated variable, whereas Elastic Net will likely keep the group and shrink them  together.
- There is difference between the correlated features and multi colinear features 
