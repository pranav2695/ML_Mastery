# Machine Learning 
- It is the field of study that gives the ability to learn without beign explicitely programmed.
- There are two main type of machine learning algorithms
    * Supervised Algorithm
    * Unsupervised Algorithm
    * we also have Recommender Systme
    * Rienforcement Learning
## Supervised Learning
- Basically Algo learns from the right answers i.e for the input variables X we have the correct output i.e Y
-             X -> Y
- One of the algo of supervised learning is regression which predicting the a number from the infinte number of possibilty.
- we have classification problems where we try to predict from small number of possible outcomes. Here Outcomes can be called as Class or Category.
## Unsupervised Learning
- we dont have right answers or labels, we want to find the patterns or structure itself.
- Here we have clustering for eg Google News, Market segmentation, Grouping Customers, Find something interesting.

## Linear Regression
- when we want to predict the values in data set. we are fitting the linear regression line.
### Classification Model
In the classification model we predict category or class. we have train our model on the training set with right label then we will predict the Target variable or output variable.  
 m -> Total number of Training example   
$(x,y)$ -> single training example  
$x^{(i)} - y^{(i)}$ = $(i)^th$ -> Training example  
$$f_{wb}(x) = wx + b$$
cost function $$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$

if we calculate the partial dev of the cost function then we have two methods  
1. Gradient descent algo  
$$w = w - \alpha \frac{\partial J}{\partial w}$$
$$b = b - \alpha \frac{\partial J}{\partial b}$$
2. Ordinary Least Squares (OLS)
  
if alpha is too small then it will learn very slowly or if alpha is too large then it will oscillates between the min an max value of the cost fucntion. Also as we start reaching the local minima parameters value will not change as frquently because derivative become small in each step

In Gradient Descent we use the algo to iterative reach the optimum point   
In OLS we mathmatically cal the values of w, b which is only suitable for the smaller data set only.

### Gradient Descent
For the linear regression model we dont have to worry about the local minima during the optimization because our cost function which is mean squared error is a convex function like a bowl   
There are basically two types of Gradient Descent algo
1. Batch GD: Entire dataset 
2. Stochastic GD : 1 random exmaple. High Noise and never fully settles
3. Mini Batch GD : Small subset it balance the speed and stability. Requires tuning the batch size hyper parameters
Note: we have also have the cat based on the optimization on Momentum or adaptive learning rate which are generally used in Neural network will discuss these there only
