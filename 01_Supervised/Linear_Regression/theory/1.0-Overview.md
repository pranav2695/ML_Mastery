# Machine Learning 
- It is the field of study that gives the ability to learn without being explicitely programmed.
- There are two main type of machine learning algorithms
    * Supervised Algorithm
    * Unsupervised Algorithm
    * we also have Recommender System
    * Rienforcement Learning
## Supervised Learning
- Basically Algo learns from the right answers i.e for the input variables X we have the correct output i.e Y
-             X -> Y
- One of the algo of supervised learning is regression which predicting the a number from the infinte number of possibilty.
- we have classification problems where we try to predict from small number of possible outcomes. Here Outcomes can be called as Class or Category.
## Unsupervised Learning
- we dont have right answers or labels, we want to find the patterns or structure itself.
- Here we have clustering for eg Google News, Market segmentation, Grouping Customers, Find something interesting.

## Linear Regression
- when we want to predict the values in data set. we are fitting the linear regression line.
### Classification Model
In the classification model we predict category or class. we have train our model on the training set with right label then we will predict the Target variable or output variable.  
 m -> Total number of Training example   
$(x,y)$ -> single training example  
$x^{(i)} - y^{(i)}$ = $(i)^th$ -> Training example  
$$f_{wb}(x) = wx + b$$
cost function $$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$

if we calculate the partial dev of the cost function then we have two methods  
1. Gradient descent algo  
$$w = w - \alpha \frac{\partial J}{\partial w}$$
$$b = b - \alpha \frac{\partial J}{\partial b}$$
2. Ordinary Least Squares (OLS)
  
if alpha is too small then it will learn very slowly or if alpha is too large then it will oscillates between the min and max value of the cost fucntion. Also as we start reaching the local minima parameters value will not change as frquently because derivative become small in each step

In Gradient Descent we use the algo to iterative reach the optimum point   
In OLS we mathmatically cal the values of w, b which is only suitable for the smaller data set only or input features less than 10000.

### Gradient Descent
For the linear regression model we dont have to worry about the local minima during the optimization because our cost function which is mean squared error is a convex function like a bowl   
There are basically two types of Gradient Descent algo
1. Batch GD: Entire dataset 
2. Stochastic GD : 1 random exmaple. High Noise and never fully settles
3. Mini Batch GD : Small subset it balance the speed and stability. Requires tuning the batch size hyper parameters
Note: we have also have the cat based on the optimization on Momentum or adaptive learning rate which are generally used in Neural network will discuss these there only
### Vectorization 
It is the technique where we optimize the commputation, as the number of variable increase we should use the vectorization to calculate the performance. 
$$f(x) = np.dot(w,x) + b$$
### Practial tricks and tips
1. Feature Scaling : when value of feature is large then parameter w will choose smaller value and if value of feature is small then it will choose large value. As the value of feature is very less ie small changes in the value can change the cost function a lot.
To solve this type of issue we did the feature scaling    
    a. Max scaling : we will divide the feature with the max value of the feature  
    b. Mean scaling : $(x_i - \mu)/(x_{max} - x_{min}) $   
    c. z-score scaling : $(x_i - \mu )/\sigma$    
2. To make sure that Gradient descent is working correctly or not, we use the learning curve. choosing number of iteration in advance is not possible we need to check the graph or automatic covergance task
3. we can choose the very small learning rate and still the cost is changing then there is bug in the code
3. To select the value of learning rate $\alpha$ we choose the very small value and keep multiple the value by 3X and then plot the graph. In the line where cost is rapidely decreaing we will choose the value of alpha $\alpha$
 